---
title: "Statistical Methods in R"
output: html_notebook
editor_options: 
  chunk_output_type: inline
---


## Agenda

- review of hypothesis testing and confidence intervals
- odds ratios
- linear model basics
- logistic regression basics
- survival analysis basics

That's a lot for 2 hours!

## Load data and packages

```{r}
library(tidyverse)
```


Today we'll continue working with the following data:

Jung, Su-Young J et al. (2019), Data from: Phosphate is a potential biomarker of disease severity and predicts adverse outcomes in acute kidney injury patients undergoing continuous renal replacement therapy, Dryad, Dataset, https://doi.org/10.5061/dryad.6v0j9

```{r}
d <- readRDS('crrt.rds')
summary(d$Death_90D)
```

We need to derive a new variable to indicate survivors and non-survivors. For this we'll use the `Death_90D` variable.

```{r}
d$survivor <- ifelse(d$Death_90D == "No", "Survivor", "Non-Survivor")
table(d$survivor)
```


## Hypothesis testing

Usually used to assess if two statistical measures are likely different, or if some statistic is likely different from 0. For comparing two groups, it's usually stated like so:

- Null: no difference in groups
- Alternative: groups are different

This is called a _two-sided_ test since our alternative does not specify which group is less than or more than the other. 

Example: Compare proportion of survivors and non-survivors with myocardial infarction.

```{r}
xtabs(~ survivor + MI, data = d) %>% 
  proportions(margin = 1) %>% 
  round(2) 
```

About 9% of survivors experienced MI versus 11% of non-survivors. How likely are we to observe a difference as big or bigger than 0.02 if there really is no difference in the entire population?

The answer is a _p-value_. We get the p-value using a hypothesis test. In this case we want to use a 2-sample proportion test with a two-sided alternative.

## 2-sample proportion test in R

We use `prop.test` function. It requires the number of "successes" (ie, event of interest) and the number of "trials" (ie, total number in each group).

Let's use `xtabs` and `addmargins` to get counts and row totals:

```{r}
xtabs(~ survivor + MI, data = d) %>% 
  addmargins()
```

Now we can use those values with `prop.test`:

```{r}
prop.test(x = c(76, 36), n = c(821, 323))
```

## Interpreting the test results

The p-value is about 0.39. If the proportion of survivors and non-survivors with MI truly was equal, this result is not unexpected. There's about a 39% chance of getting two proportions as different as these (or more different).

We could say, "with the current sample size the data were unable to overcome the supposition of no difference in the proportions." 

The 95% confidence interval is [-.06, 0.02]. We're not sure of the magnitude or direction of the difference.

More about confidence intervals shortly.

## Chi-square test

Another way to compare two proportions is with a chi-square test of association. The null hypothesis is no association between survivor status and MI. Notice the p-value is identical to what we got with `prop.test`. In fact the math is exactly the same. 

```{r}
xtabs(~ survivor + MI, data = d) %>% 
  chisq.test()
```

The result of the `prop.test` function is more informative about how the proportions may differ. 

## Comparing our results with Table 1

The p-value reported for this comparison in Table 1 of the article is 0.20. Why is that? Apparently their hypothesis was as follows:

- Null: no difference in proportions
- Alternative: non-survivors less than survivors

To replicate their result we specify `alternative = "less"`

```{r}
prop.test(x = c(76, 36), n = c(821, 323), alternative = "less")
```

This test is making a stronger alternative assumption that the proportion of non-survivors with MI is _less than_ the proportion of survivors with MI.

Editorial: The article does not explain why they used this alternative. I'm surprised they used it. Usually Table 1 comparisons use the more conservative two-sided alternative. It is important to note that you _DO NOT_ pick the alternative based on the observed proportions!

## More hypothesis tests

Base R has a number of hypothesis test functions. Here are a few:

- `prop.test`: compare two proportions
- `t.test`: compare two means
- `chisq.test`: association between categorical variables
- `cor.test`: linear association between numeric variables
- `wilcox.test`: compare ranks between two groups
- `kruskal.test`: compare ranks between more than two groups

Much could be said about the theory and assumptions of these tests. We don't have time to do that. Besides most research questions require more sophisticated statistical approaches that allow us to incorporate multiple variables or predictors. 

However here are two quick examples.

## Mann-Whitney test (aka Wilcoxon test)

From the article: "Variables that did not show normal distribution were compared using Mann-Whitney test or Kruskal-Wallis test. The Kolmogorov-Smirnov test was used to examine the normality of the distribution of parameters."

Let's look at White Blood Cell count. These values are reported in Table 1.

```{r}
tapply(d$WBC_0h, d$survivor, summary)
```

They decided to report medians instead of means, so presumably WBC is not normally distributed. The authors said they used the Kolmogorov-Smirnov test for normality. The null is the data are normal. A small p-value provides evidence against the null. Below we use `tapply` to apply the `ks.test` function to WBC_0h by survivor group. We specify "dnorm" because we want to compare the distribution of WBC to a normal distribution. 

```{r}
tapply(d$WBC_0h, d$survivor, ks.test, "dnorm")
```

It's probably more instructive to simply look at histograms. We don't really need a hypothesis test to sanctify the fact the distributions are not normal.

```{r}
ggplot(d) +
  aes(x = WBC_0h) +
  geom_histogram() +
  facet_wrap(~survivor)
```

Since these distributions are not normal looking, the authors elected to compare the medians instead of the means using the Mann-Whitney test, aka Wilcoxon test.

```{r}
wilcox.test(WBC_0h ~ survivor, data = d)
```

## Correlation test

From the article: "Phosphate level significantly correlated with the APACHE II (P < 0.001) and SOFA (P < 0.001) scores."

Correlation summarizes the strength and direction of a linear relationship. A correlation test tests the null hypothesis that the correlation is different from 0.

```{r}
cor.test(d$P_0h, d$APA_II_CRRT_0h)
```

The paper simply reports the p-value of this test is less than 0.001. 

Editorial: I think it would have been more instructive to report the estimated correlation and confidence interval: 0.18 [0.12, 0.24]. A scatterplot makes this "significant" correlation appear much less significant.

```{r}
ggplot(d) +
  aes(x = P_0h, y = APA_II_CRRT_0h) +
  geom_point() +
  geom_smooth(method = "lm")
```


## Complaints about hypothesis testing

- implausible null hypotheses
- encourages researchers to engage in dichotomous thinking and downplay
  uncertainty and practical significance
- misinterpretation of the p-value
    - the probability that the null hypothesis is true **(FALSE)**
    - the probability that the alternative hypothesis is NOT true **(FALSE)** 
    - smaller p-values mean bigger effects **(FALSE)**
    
## Confidence intervals

A statistic calculated from a sample is just an estimate. Another sample will almost certainly yield a different estimate. A confidence interval gives us some understanding of how uncertain we are about the estimate.

Confidence intervals are _not probability intervals_. 

The "confidence" is in the process:

1. sample data
2. calculate a 95% confidence interval
3. repeat steps 1 and 2 many times.
4. about 95% of the confidence intervals will contain the "true" value

Quick demo

```{r}
# sample 30 obs from a Normal distribution with mean 25 and SD 3
x <- rnorm(n = 30, mean = 25, sd = 3)
# use t.test function to get 95% CI and save
t.out <- t.test(x)
# extract CI and see if it contains the true mean of 25
t.out$conf.int[1] < 25 & t.out$conf.int[2] > 25
```

We can do this 1000 times with the `replicate` function and save the results. We use `{}` to capture more than one line of code. 

```{r}
results <- replicate(1000, expr = {
  x <- rnorm(n = 30, mean = 25, sd = 3)
  t.out <- t.test(x)
  t.out$conf.int[1] < 25 & t.out$conf.int[2] > 25})

# mean of TRUE/FALSE (1,0) is proportion of TRUEs
mean(results)
```

The point is that 95% confidence intervals either do or do not contain the "true" value. The process results in CIs capturing the value about 95% of the time.

BTW, there's nothing special about "95%". _That's just tradition._ 

Confidence intervals quantify uncertainty and tell us about _the magnitude and direction_ of an effect in the context of a hypothesis test.

## CI Example

The article reports a significant difference in mean APACHE II scores between survivors and non-survivors (Table 1). We can replicate this using the `t.test` function. However it does not report a confidence interval on the difference in means.

```{r}
t.test(APA_II_CRRT_0h ~ survivor, data = d)
```

Notice the confidence interval is (2.2, 4.3). We're pretty confident the mean difference is somewhere in that range. 

Editorial: The internet tells us that "The APACHE II score ranges from 0 to 71 points; however, it is rare for any patient to accumulate more than 55 points." I'm not sure how "significant" a difference of 2-4 points is. 

## Odds and Odds Ratios

Odds ratios are frequently used in medical research. They often arise in the context of logistic regression, which is used to model the probability of an event given one or more predictors.

Odds are a function of probability.

$$O = \frac{p}{(1 - p)} $$
If p = 0.75, then the odds are 3 to 1.

```{r}
0.75/(1 - 0.75)
```

That's 3 successes for every failure.

If p = 0.25, then the odds are 3 to 1 against, or 1/3.

```{r}
0.25/(1 - 0.25)
```

That's 1 success for every 3 failures.

Notice odds are positive and can range from [0, infinity) unlike probability which ranges [0,1].

**Odds ratios** are simply the ratio of two odds.

