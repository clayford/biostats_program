---
title: "Statistical Methods in R"
output: html_notebook
editor_options: 
  chunk_output_type: inline
---

## Agenda

- review of hypothesis testing and confidence intervals
- odds ratios
- linear model basics
- logistic regression basics
- survival analysis basics

That's a lot for 2 hours!

## Hypothesis testing

Usually used to assess if two statistical measures are likely different, or if some statistic is likely different from 0.

Example: I randomly sample 100 UVA females and 100 UVA males and ask if they floss.

- Female: 34/100 = 0.34
- Male: 28/100 = 0.28

How likely are we to observe a difference as big or bigger than 0.06 if there really is no difference in the entire population?

The answer is a _p-value_.

## Running the test in R

```{r echo=TRUE}
prop.test(x = c(34, 28), n = c(100, 100))
```

## Interpreting the test results

The p-value is about 0.44. If the proportion of males and females who floss truly was equal, this result is not unexpected. There's about a 44% chance of getting two proportions as different as these (or more different).

We could say, "with the current sample size the data were unable to overcome the supposition of no difference in the proportions." 

The 95% confidence interval is [-.07, 0.20]. We're not sure of the magnitude or direction of the difference.

More about confidence intervals shortly.

## More hypothesis tests

Base R has a number of hypothesis test functions. Here are a few:

- `prop.test`: compare two proportions
- `t.test`: compare two means
- `chisq.test`: association between categorical variables
- `cor.test`: association between numeric variables
- `wilcox.test`: compare rank of means

Much could be said about the theory and assumptions of these tests. We don't have time to do that. Besides most research questions require more sophisticated statistical approaches that allow us to incorporate multiple variables or predictors. 

However two quick demos with some semi-realistic data.

## Two examples

The following reads in a data frame containing information on chronic kidney disease. Downloaded from the UCI Machine Learning Repository. https://archive.ics.uci.edu/ml/datasets/chronic_kidney_disease

An "rds" file is a single R object. It's an efficient way to save R data frames. See this script for how I downloaded and processed the file.

The data frame has one record per subject. The `class` column tells us whether the subject has chronic kidney disease or not

```{r}
kd <- readRDS('kidney_disease.rds')
table(kd$class)
```


### T-test

The `wbcc` column provides white blood cell count. Is there a difference in mean white blood cell count between subjects with and without chronic kidney disease? It looks like there is.

```{r}
tapply(kd$wbcc, kd$class, summary)
```

We can investigate with a t-test.

```{r}
t.test(wbcc ~ class, data = kd)
```

Null: no difference in mean wbcc
Alternative: some difference

We have good evidence that mean wbcc is higher in subjects with chronic kidney disease. The 95% CI on the difference in means suggests the difference is at least 714.

### Correlation test

Is there a linear association between Hemoglobin and Red blood cell count for subjects with chronic kidney disease? It sure looks like it.

```{r}
plot(hemo ~ rbcc, data = kd, subset = class == "ckd")
```

We can summarize linear relationships with correlation.

```{r}
cor.test(~ rbcc + hemo, data = d, subset = class == "ckd")
```

It appears the correlation is somewhere between 0.70 and 0.84. The hypothesis test is comparing it to 0. 

## Complaints about hypothesis testing

- implausible null hypotheses
- encourages researchers to engage in dichotomous thinking
- misinterpretation of the p-value
    - the probability that the null hypothesis is true **(FALSE)**
    - the probability that the alternative hypothesis is NOT true **(FALSE)** 
    - smaller means a bigger effect **(FALSE)**


## Confidence intervals

A statistic calculated from a sample is just an estimate. Another sample will almost certainly yield a different estimate. A confidence interval gives us some understanding of how uncertain we are about the estimate.

Confidence intervals are not probability intervals. 

The "confidence" is in the process:

1. sample data
2. calculate a 95% confidence interval
3. repeat steps 1 and 2 many times.
4. about 95% of the confidence intervals will contain the "true" value

Quick demo

```{r}
# sample 30 obs from a Normal distribution with mean 25 and SD 3
x <- rnorm(n = 30, mean = 25, sd = 3)
# use t.test function to get 95% CI and save
t.out <- t.test(x)
# extract CI and see if it contains the true mean of 25
t.out$conf.int[1] < 25 & t.out$conf.int[2] > 25
```

We can turn this into a function and run many times.

```{r}
f <- function(n, mean, sd, cl){
  x <- rnorm(n = n, mean = mean, sd = sd)
  t.out <- t.test(x, conf.level = cl)
  t.out$conf.int[1] < mean & t.out$conf.int[2] > mean
}

f(n = 30, mean = 25, sd = 3, cl = 0.95)

results <- replicate(1000, expr = f(n = 30, mean = 25, sd = 3, cl = 0.95))
sum(results)
mean(results)
```

The point is that 95% confidence intervals either do or do not contain the "true" value. The process results in CIs capturing the value about 95% of the time.

BTW, there's nothing special about "95%". That's just tradition. 

Confidence intervals quantify uncertainty and tell us about the magnitude and direction of an effect in the context of a hypothesis test.

## CI Example

Is there a difference in mean Serum Creatinine between subjects with and without chronic kidney disease? It seems that way.

```{r}
aggregate(sc ~ class, data = kd, mean)
```

Let's use the `t-test` function to get a 95% CI on the difference in means.

```{r}
t.test(sc ~ class, data = kd)
```

The p-value is tiny, but doesn't really tell us much about how different the means are. That information is in the 95% confidence interval of the difference in means. It appears the "ckd" group has a mean that's at least 2.7 higher.


## Odds and Odds Ratios

Odds ratios are frequently used in medical research. They often arise in the context of logistic regression, which is used to model the probability of an event given one or more predictors.

Odds are a function of probability.

$$O = \frac{p}{(1 - p)} $$
If p = 0.75, then the odds are 3 to 1.

```{r}
0.75/(1 - 0.75)
```

That's 3 successes for every failure.

If p = 0.25, then the odds are 3 to 1 against, or 1/3.

```{r}
0.25/(1 - 0.25)
```

That's 1 success for every 3 failures.

Notice odds are positive and can range from [0, infinity) unlike probability which ranges [0,1].

**Odds ratios** are simply the ratio of two odds.

Back to our flossing example.

```{r}
f_odds <- 0.34 / (1 - 0.34)
m_odds <- 0.28 / (1 - 0.28)
c(f_odds, m_odds)
```

The odds ratio:

```{r}
f_odds/m_odds
```

The odds that a female flosses is about 32% higher than the odds that a male flosses.

Going the other way:

```{r}
m_odds/f_odds
```

The odds that a male flosses is about 25% lower than the odds that a female flosses.

An odds ratio of 1 is equivalent to no difference in odds.

## Odds ratio example

The paper entitled "Race Is a Major Determinant of Secondary Hyperparathyroidism in Uremic Patients" (in J Am Soc Nephrol 11: 330â€“334, 2000) calculates the risk of severe secondary hyperparathyroidism between black and white subjects using odds ratios (Table 5). "Severe" is determined to be mean parathyroid hormone (PTH) > 500 pg/ml.  (Picograms per milliliter; A picogram is one-trillionth of a gram)

Here's the data:

```{r}
tab5 <- matrix(c(193, 219, 9, 45), 
             ncol = 2, 
             dimnames = list(c("white", "black"),
                             c("mean PTH <500", "mean PTH >500")))
tab5
```

Let's get estimated probabilities. About 0.17 versus 0.04.

```{r}
t5 <- proportions(tab5, margin = 1)
t5
```

Let's get the odds. About 0.05 versus 0.21.

```{r}
w_odds <- t5[1,2]/(1 - t5[1,2])
b_odds <- t5[2,2]/(1 - t5[2,2])
round(c(w_odds, b_odds), 2)
```

The odds ratio:

```{r}
b_odds/w_odds
```

It appears the odds (or risk) of severe secondary hyperparathyroidism in a black subject is about 4.4 times higher than the odds of a white subject.

This odds ratio is just an estimate. Let's calculate a 95% CI for the odds ratio. For this we can use the `oddsratio` function in the epitools package.

```{r}
library(epitools)
oddsratio(tab5, method = "wald") # reproduces result in paper
```

It appears the odds are at least 2 times higher for black than white, perhaps as much as 9 times higher. 

## Pro and con of odds ratios

A pro of odds ratios is that they're constant over the range of probability [0,1]. 

Consider the following probabilities and their difference:

```{r}
p1 <- c(0.2, 0.5, 0.8, 0.9, 0.98)
p2 <- c(0.33, 0.67, 0.89, 0.95, 0.99)
cbind(p1, p2, riskdiff = p2 - p1)
```

But notice the odds ratios are virtually constant.

```{r}
cbind(p1, p2, riskdiff = p2 - p1, 
      oddsratio = (p2/(1-p2))/(p1/(1-p1)))

```

This is useful for understanding the effect of some risk factor in a model.

On the other hand, a con of odds ratios is that they obscure the difference in probabilities!

Here's an odds ratio of about 9 that reflects an increase from 0.001 to 0.009. 

```{r}
(0.009/(1 - 0.009)) /
  (0.001/(1 - 0.001))
```

Here's another odds ratio of about 9 that reflects an increase from 0.07 to 0.40.

```{r}
(0.4/(1 - 0.4)) /
  (0.07/(1 - 0.07))

```

An odds ratio of 9 by itself doesn't tell us about absolute risk. Increasing the risk of illness from 0.001 to 0.009 is probably not as concerning as increasing the risk of illness from 0.07 to 0.40.

Carefully consider all statistics. 


## Statistical models

Why do some people have chronic kidney disease and others do not? 
Why do uremic patients have different parathyroid hormone (PTH) levels?

That's much of statistics in nutshell: understanding variability.

**Statistical models** attempt to understand variability in some outcome by modeling it as a function of other variables. That's a fancy way of saying they allows us to calculate _conditional means_.

Let's say we model PTH levels as a function of race, age, gender, calcium level, and interdialytic weight gain. That would allow us to do things like...

1. calculate the expected mean PTH level for, say, a white 40-year old female with 9 mg/dl calcium and 2 lbs of interdialytic weight gain 
2. quantify how race affects expected mean PTH level
3. determine whether age and gender both affect expected mean PTH level

Statistical modeling is a massive subject! We will briefly discuss two types of models: linear modeling and logistic regression


## Linear models

Linear models, or multiple regression, seek to explain the variability of a _numeric_ outcome as a weighted sum of predictors. For this reason we call them additive models. 

T-tests, ANOVAs, and ANCOVAs are all special cases of linear modeling.

Let's use some simulated data to work with linear models. First we simulate data for a straight line. Notice y is completed determined by x. For every one unit increase in x, y increases by 0.5.

```{r}
x <- 1:25
y <- 2 + 0.5*x
plot(x, y)
```

Now let's some generate some "noise" and add that to the data. We'll draw 25 values from a Normal distribution with mean 0 and standard deviation 3.

```{r}
set.seed(1)
e <- rnorm(25, mean = 0, sd = 1.5)
y <- y + e
plot(x, y)
```

Now let's fit a model to this data. Since we generated the data, we know the "correct" model. We use the `lm` function. "y ~ x" means "model y as a function of x plus an intercept". We save the result to `m` and use the `summary` function to look at the results. 

```{r}
m <- lm(y ~ x)
summary(m)
```

Recall the "true" values we used to generate this data were

- Intercept: 2
- Slope: 0.5
- SD of Normal dist'n: 1.5

The `lm` function essentially tries to recover these values. It estimates...

- Intercept: 2.170
- Slope: 0.506
- SD of Normal dist'n: 1.455 (Residual standard error)

`lm` took the straight line model we gave it (y = a + bx), looked at the data, and estimated it came from a line with formula y = 2.17 + 0.50*x and noise/error drawn from a Normal distribution with mean 0 and standard deviation 1.45.

_That's basically statistical modeling: propose a model that you think captures the data generating mechanism and fit it to the data._

Notice `lm` assumes the noise comes from a Normal distribution with mean 0 and a fixed standard deviation. Those are known as the _normality_ and _constant variance_ assumptions. It also assumes data are _independent_ of one another and the dependent variable (y) is _continuous_. (ie, numbers with decimals)

More advanced models allow for non-continuous dependent variables, non-constant variance, non-normal error distributions, non-linear effects, interactions, non-independent observation, etc.

Let's look at the summary again:

```{r}
summary(m)
```

The _Residuals_ section is a way to assess the constant variance assumption. Hopefully median is close to 0 and the Min/1Q and 3Q/Max values are similar in absolute value

The _Coefficients_ section contains the estimated weights in our weighted sum. The standard errors quantify the uncertainty in our estimated. The t value and associated p-values are null hypothesis tests that the coefficients are 0.

The _R-squared_ values quantify how much variability in our dependent variable is explained by the predictors. Ranges from 0 to 1.

The _F-statistic_ is a null hypothesis test that all predictors (except intercept) are 0.

Let's do a realistic example. The vitcap2 data frame has 84 rows and 3 columns and contains information on workers in the cadmium industry. Of interest is understanding lung volume as a function of age and length of exposure to cadmium. The internet tells me that "Cadmium is a naturally occurring toxic metal."

```{r}
library(ISwR)
data("vitcap2")
summary(vitcap2)
```

The group variable is not really numeric but rather codes: 

- 1: exposed > 10 years 
- 2: exposed < 10 years
- 3: not exposed

Let's make that a factor.

```{r}
vitcap2$group <- factor(vitcap2$group, 
                        labels = c("exposed > 10 years", 
                                   "exposed < 10 years",
                                   "not exposed"))
summary(vitcap2)
```

Quick plot

```{r}
library(ggplot2)
ggplot(vitcap2) +
  aes(x = age, y = vital.capacity) +
  geom_point() +
  facet_wrap(~group)
```


Model vital.capacity as a function of group and age. We are assuming that vital.capacity can be modeled as a weighted sum of age and exposure group.

```{r}
mod1 <- lm(vital.capacity ~ group + age, data = vitcap2)
summary(mod1)
```

_If you believe this model_, it appears that as you age, your lung volume decreases; about 0.03 every year you get older. This model assumes the effect of age is the same, no matter the length of exposure.

What if we thought the effect of age _depended_ on exposure? To model this we need to include an interaction.

```{r}
mod2 <- lm(vital.capacity ~ group + age + group:age, data = vitcap2)
summary(mod2)

```

Our model is now getting difficult to interpret. The age coefficient of -0.08 now applies to the group exposed more than 10 years. The last two rows reflect the _change in the age coefficient_ for the other two groups. It appears the "not exposed" group has an age effect of about -0.08 + 0.05 = -0.03.

This model is sometimes referred to as an ANCOVA model, or analysis of covariance. 

Which model is better? Or in other words, do we need the interaction? We can evaluate this with a hypothesis test called the "partial F test". The null hypothesis is that there is no difference in the models in explaining the variability in vital.capacity. 

```{r}
anova(mod1, mod2)
```

It appears we have some evidence against the null and that the more complicated model may be justified.

Effect plots can be helpful for visualizing these models. The ggeffects package is one package that makes these plots relatively easy to create.

```{r}
library(ggeffects)
plot(ggeffect(mod2, terms = c("age", "group")))
```

It looks the effect of age is more pronounced for those exposed to cadmium over 10 years. But notice the large confidence ribbon around age 20. Is it possible to have someone age 20 that has been exposed to cadmium over 10 years?

What if we want to use our model to make a prediction? We can use the `predict` function for this. Expected mean vital capacity for someone age 40 and exposed over 10 years? 

```{r}
predict(mod2, newdata = data.frame(age = 40, group = "exposed > 10 years"))
```

This is just an estimate. Let's get a 95% confidence interval on the _expected mean_:

```{r}
predict(mod2, newdata = data.frame(age = 40, group = "exposed > 10 years"),
        interval = "confidence")
```

If we wanted a confidence interval for a predicted value for a specific person age 40 and exposed over 10 years we use a "prediction" interval.

```{r}
predict(mod2, newdata = data.frame(age = 40, group = "exposed > 10 years"),
        interval = "prediction")

```

Notice it's much wider. Much less certain about expected values for a _person_ versus an expected _mean_ value.

Advice: go slow, be cautious, and be modest when doing statistical modeling.

## Logistic regression

We often want insight into why a certain event happens or does not happen. Chronic kidney disease is one such event. Are there certain predictors that might help us assess the probability of having or developing chronic kidney disease?

Logistic regression is a valuable method for _modeling the probability_ of binary events. Given various predictor inputs it returns a predicted probability that ranges from 0 to 1. 

Whole books are devoted to logistic regression. This is a very brief intro!

Let's dive into an example. Model the probability of chronic kidney disease as a function of blood glucose and blood urea.

Let's first examine the variables we'll use:

```{r}
by(kd[,c("bu", "bgr")], kd$class, summary)
```

We have some missing data. A modern statistical approach is to _impute_ the missing data using sophisticated methods. In the interest of time we'll simply use what data we have.

```{r}
kd2 <- na.omit(kd[,c("bu", "bgr","class")])
```


Notice we use `glm` instead of `lm`. Logistic regression belongs to a class of models known as Generalized Linear Models. Notice we also specify the error distribution as "binomial", to indicate our response is binary in nature. Finally, before we start we relevel the class variable to have "notckd" as the reference level. That means we'll model the probability of having kidney disease.

```{r}
kd2$class <- relevel(kd2$class, ref = "notckd")
lr1 <- glm(class ~ bgr + bu, data = kd2, family = "binomial")
summary(lr1)
```


The coefficients are on the _log_odds_ scale. They are difficult to interpret, though positive coefficients mean the increase the probability of the event as they get higher. If we exponentiate the coefficients we get an _odds ratio_. Those are a little easier to interpret and are usually reported in articles.

```{r}
exp(coef(lr1))
```

It appears that every one unit increase in these predictors increases the odds of kidney disease diagnosis by about 3 and 5 percent, respectively. 

To get confidence intervals we can use the `confint` funtion and then exponentiate.

```{r}
exp(confint(lr1))
```

The odds ratio (OR) for blood urea might be report as 1.05 (1.04, 1.07).

The `predict` function allows us to get predicted probabilities.

```{r}
predict(lr1, newdata = data.frame(bgr = 120, bu = 40), type = "response")
```

Effect plots are useful for understanding logistic regression models.

```{r}
plot(ggeffect(lr1))
```


How good is this model? _Deviance_ helps answer this. Deviance summarizes the difference between observed and fitted values. The deviance by itself means little.

```{r}
deviance(lr1)
```

However if the model is "good" the deviance should be distributed as a chi-square with n - p degrees of freedom. Our model `lr1` has 348 degrees of freedom. What does that mean? Here's what a chi-square distribution with 348 DF looks like.

```{r}
curve(dchisq(x, df = 348), from = 250, to = 450)
```

Our residual deviance should like it could have plausibly been drawn from this distribution. What's the probability of getting 291.72 or higher from this distribution?

```{r}
pchisq(deviance(lr1), df = df.residual(lr1), lower.tail = FALSE)
```

Since this p-value is large, we may conclude this model fits good enough (though not necessarily "correct"). You don't always need a p-value. If deviance is way bigger than the degrees of freedom, you can safely conclude the model isn't "good".

A pseudo R-squared can be calculated using the `nagelkerke` function in the rcompanion package.

```{r}
rcompanion::nagelkerke(lr1)
```


## Survival analysis basics

Survival analysis is the analysis of data on _the time to occurrence of a specific event_. Example: time to receive a kidney transplant once a subject is placed on the transplant list. 

The term "survival" is used because the event of interest is often (sadly) death. Survival analysis is a field unto itself. I can't do it justice in this little presentation.

We use survival analysis to estimate _the probability that a subject does not experience the event until some time_. Example: Probability a subject waits at least 6 months to get a transplant is about 0.70.

An important feature of survival analysis data is that it usually contains "incomplete" or _censored_ data. These are subjects that do not experience the event of interest. Example: some subjects on a kidney transplant list do not  receive a transplant _during the observation period_. However we can make use of their data. To exclude them would _bias_ our probability estimates, which is a fancy way of saying it would make our estimates wrong. 





## References

-  Blakeley B. McShane, David Gal, Andrew Gelman, Christian Robert & Jennifer L. Tackett (2019) Abandon Statistical Significance, The American Statistician, 73:sup1, 235-245, DOI: 10.1080/00031305.2018.1527253 
